{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Log Classification\n",
    "\n",
    "This notebook adapts the Scikit Learn Log Classification notebook for Keras by using neural networks with TensorFlow.\n",
    "\n",
    "We still use Scikit Learn for some functions, like labelling data and providing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data(src_file_path, dst_file_path):\n",
    "    if not os.path.exists(dst_file_path):\n",
    "        os.mkdir(dst_file_path)\n",
    "    for logfile in glob.glob(src_file_path + \"/*.log\"):\n",
    "        if os.stat(logfile)[6] > 10000:\n",
    "            logfile_name = logfile.split('/')[-1]\n",
    "            shutil.copyfile(logfile, dst_file_path + \"/\" + logfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(logfile_path):\n",
    "    log_collection = pd.DataFrame()\n",
    "    logs = pd.DataFrame()\n",
    "    logfiles = glob.glob(logfile_path + \"/*.log\") # Get list of log files\n",
    "    for logfile in logfiles:\n",
    "        logs = pd.read_csv(logfile, sep=\"\\n\", header=None, names=['data'])\n",
    "        logs['type'] = logfile.split('/')[-1]\n",
    "        # Add log file data and type to log collection\n",
    "        log_collection = log_collection.append(logs)\n",
    "\n",
    "    # Remove empty lines\n",
    "    log_collection = log_collection.dropna()\n",
    "    # Reset the index\n",
    "    log_collection = log_collection.reset_index(drop=True)\n",
    "    \n",
    "    return log_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text, labels):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    X = tokenizer.texts_to_matrix(text, mode='tfidf')\n",
    "        \n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(labels)\n",
    "    y = encoder.transform(labels)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(input_size, hidden_size, num_classes, dropout):\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(hidden_size, input_shape=(input_size,)))\n",
    "    nn.add(Activation('relu'))\n",
    "    nn.add(Dropout(dropout))\n",
    "    nn.add(Dense(num_classes))\n",
    "    nn.add(Activation('softmax'))\n",
    "    nn.summary()\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, criterion, optimiser, batch_size, num_epochs):\n",
    "    network.compile(loss=criterion,\n",
    "                  optimizer=optimiser,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = network.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.1)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(actual, predictions):\n",
    "    print(\"\\033[1m Performance Report \\033[0m\\033[50m\\n\")\n",
    "    \n",
    "    actual = np.array(actual)\n",
    "    \n",
    "    print(confusion_matrix(actual, predictions))\n",
    "    print\n",
    "    print(classification_report(actual, predictions))\n",
    "    print(\"Accuracy: \" + str(round(accuracy_score(actual, predictions),2)))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_dir = \"/var/log\"\n",
    "data_dir = \"data\"\n",
    "\n",
    "copy_data(source_data_dir, data_dir)\n",
    "log_collection = read_data(data_dir)\n",
    "\n",
    "X, y = prepare_data(log_collection['data'], log_collection['type'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = X_train.shape[1] # this is the vocab size\n",
    "hidden_size = 512\n",
    "num_classes = y_train.shape[1]\n",
    "dropout = 0.3\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               5273600   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 3591      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 5,277,191\n",
      "Trainable params: 5,277,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = build_nn(input_size, hidden_size, num_classes, dropout)\n",
    "criterion = 'categorical_crossentropy'\n",
    "optimiser = Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63326 samples, validate on 7037 samples\n",
      "Epoch 1/5\n",
      "63326/63326 [==============================] - 115s 2ms/step - loss: 0.0670 - acc: 0.9802 - val_loss: 0.0278 - val_acc: 0.9886\n",
      "Epoch 2/5\n",
      "63326/63326 [==============================] - 104s 2ms/step - loss: 0.0283 - acc: 0.9868 - val_loss: 0.0257 - val_acc: 0.9888\n",
      "Epoch 3/5\n",
      "63326/63326 [==============================] - 105s 2ms/step - loss: 0.0275 - acc: 0.9871 - val_loss: 0.0261 - val_acc: 0.9883\n",
      "Epoch 4/5\n",
      "63326/63326 [==============================] - 112s 2ms/step - loss: 0.0273 - acc: 0.9871 - val_loss: 0.0275 - val_acc: 0.9883\n",
      "Epoch 5/5\n",
      "63326/63326 [==============================] - 103s 2ms/step - loss: 0.0273 - acc: 0.9869 - val_loss: 0.0281 - val_acc: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = train(X_train, y_train, criterion, optimiser, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "file_types = np.unique(log_collection['type'])\n",
    "predictions = model.predict(np.array(X_test))\n",
    "predicted_labels = [ file_types[np.argmax(p)] for p in predictions]\n",
    "actual_labels = [ file_types[np.argmax(y)] for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance Report \u001b[0m\u001b[50m\n",
      "\n",
      "[[5854    0    0    0    0    0    0]\n",
      " [   0  302    0    0    0    0    0]\n",
      " [   0    0   46    0    0    0    0]\n",
      " [   0    0    0 3588   19    0    0]\n",
      " [   0    0    0  207 2700    0    0]\n",
      " [   0    0    0    0    0  926    1]\n",
      " [   0    0    0    0    0    1 3947]]\n",
      "\n",
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "                 corecaptured.log       1.00      1.00      1.00      5854\n",
      "                    fsck_apfs.log       1.00      1.00      1.00       302\n",
      "                     fsck_hfs.log       1.00      1.00      1.00        46\n",
      "                      install.log       0.95      0.99      0.97      3607\n",
      "                       system.log       0.99      0.93      0.96      2907\n",
      "wifi-11-07-2018__13:38:02.923.log       1.00      1.00      1.00       927\n",
      "                         wifi.log       1.00      1.00      1.00      3948\n",
      "\n",
      "                        micro avg       0.99      0.99      0.99     17591\n",
      "                        macro avg       0.99      0.99      0.99     17591\n",
      "                     weighted avg       0.99      0.99      0.99     17591\n",
      "\n",
      "Accuracy: 0.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reporting\n",
    "report(actual_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
